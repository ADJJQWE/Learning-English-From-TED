# A short history of AI

In the first of six weekly briefs, we ask how AI overcame decades of underdelivering

![image-20240727160006943](./assets/image-20240727160006943.png)

原文：

Over the summer of 1956 a small but illustrious group gathered at

Dartmouth College in New Hampshire; it included Claude Shannon, the

begetter of information theory, and Herb Simon, the only person ever to win

both the Nobel Memorial Prize in Economic Sciences awarded by the Royal

Swedish Academy of Sciences and the Turing Award awarded by the

Association for Computing Machinery. They had been called together by a

young researcher, John McCarthy, who wanted to discuss “how to make

machines use language, form abstractions and concepts” and “solve kinds of

problems now reserved for humans”. It was the first academic gathering

devoted to what McCarthy dubbed “artificial intelligence”. And it set a

template for the field’s next 60-odd years in coming up with no advances on

a par with its ambitions.

1956年夏天，一个人数不多但很著名的团体聚集在新罕布什尔州的达特茅斯学院；其中包括信息论创始人Claude Shannon和Herb Simon，后者是唯一一个同时获得由皇家瑞典学院科学奖授予的诺贝尔经济学奖和由计算机械协会授予的图灵奖的人。他们是由一位年轻的研究员约翰·麦卡锡召集的，他想讨论“如何让机器使用语言，形成抽象概念”和“解决现在留给人类的各种问题”。这是第一次致力于麦卡锡所谓的“人工智能”的学术聚会。它为该领域接下来的60多年树立了一个模板，在这60多年里，它没有取得与其雄心不相上下的进步。

学习：

illustrious：美 [ɪˈlʌstriəs] 著名的；杰出的；显赫的；

begetter：美 [bɪ'ɡetə(r)] 父；生产者

memorial：美 [məˈmɔːriəl] 纪念碑；纪念像；纪念物； 纪念的；悼念的

the Nobel Memorial Prize in Economic Sciences：诺贝尔经济学奖

dub：给…起绰号；把…戏称为；授予称号

on a bar：同等；持平；堪比          

原文：

The Dartmouth meeting did not mark the beginning of scientific inquiry into

machines which could think like people. Alan Turing, for whom the Turing

prize is named, wondered about it; so did John von Neumann, an inspiration

to McCarthy. By 1956 there were already a number of approaches to the

issue; historians think one of the reasons McCarthy coined the term artificial

intelligence, later AI, for his project was that it was broad enough to

encompass them all, keeping open the question of which might be best.

Some researchers favoured systems based on combining facts about the

world with axioms like those of geometry and symbolic logic so as to infer

appropriate responses; others preferred building systems in which the

probability of one thing depended on the constantly updated probabilities of

many others.

达特茅斯会议并不标志着对能像人一样思考的机器进行科学研究的开始。图灵奖就是以艾伦·图灵的名字命名的，他对此感到疑惑；约翰·冯·诺依曼也是，他激励了麦卡锡。到1956年，已经有许多解决这个问题的方法；历史学家认为，麦卡锡为他的项目创造了人工智能(后来的AI)这一术语的原因之一是，它足够广泛，可以涵盖所有领域，并保留了哪个可能是最好的问题。一些研究人员倾向于将世界上的事实与公理(如几何学和符号逻辑)结合起来的系统，以便推断出适当的反应；其他人更喜欢建立这样的系统，其中一件事情的概率取决于许多其他事情的不断更新的概率。

学习：

inquiry：美 [ˈɪnkwəri] 探查；探询；

scientific inquiry：科学研究

coined：创造；（coin的过去分词）

coin the term：起名字

axioms：美 ['æksɪrmz]  公理；原理；定理；（axiom的复数）**注意发音**

![image-20240727162709417](./assets/image-20240727162709417.png)

原文：

The following decades saw much intellectual ferment and argument on the

topic, but by the 1980s there was wide agreement on the way forward:

“expert systems” which used symbolic logic to capture and apply the best of

human know-how. The Japanese government, in particular, threw its weight

behind the idea of such systems and the hardware they might need. But for

the most part such systems proved too inflexible to cope with the messiness

of the real world. By the late 1980s AI had fallen into disrepute, a byword for

overpromising and underdelivering. Those researchers still in the field

started to shun the term.

在接下来的几十年里，人们对这个话题进行了激烈的讨论和辩论，但到了20世纪80年代，人们对未来的发展方向达成了广泛的共识:“专家系统”，它使用符号逻辑来捕捉和应用人类知识的精华。特别是日本政府，全力支持这种系统的想法以及它们可能需要的硬件。但事实证明，这种系统在很大程度上过于僵化，无法应对现实世界的混乱局面。到了20世纪80年代末，人工智能已经声名狼藉，成了言过其实和言过其实的代名词。那些仍在该领域的研究人员开始回避这个术语。

学习：

ferment：美 [fərˈment , ˈfɜːrment] 政治或社会上的）动乱；骚动；纷扰；激烈的讨论

know-how：技能；专业技能；专业知识；

messiness：乱糟糟; 混乱

disrepute：美 [ˌdɪsrəˈpjut] 坏名声；不光彩；名誉受损；声名狼藉

byword：代名词；概括

shun：美 [ʃʌn] （故意）避开；回避；



overpromising: 承诺超出实际能做到的

underdelivering：结果未能实现预期

>解释 "Overpromising and Underdelivering"
>
>**句子的意思**：
>"Overpromising and underdelivering" 指的是承诺的超出实际能做到的，结果未能实现预期。
>
>详细解析：
>
>- **Overpromising**：做出过度承诺，指的是承诺某事会有很好的结果或达到很高的标准。
>- **Underdelivering**：未能兑现承诺，指的是实际结果未达到承诺的标准或预期。
>
>示例
>
>- **项目管理**：
>  - 例句：The project manager overpromised on the delivery date but underdelivered, causing delays and dissatisfaction.
>  - 翻译：项目经理在交付日期上做出了过度承诺，但未能兑现，导致了延误和不满。
>
>- **产品发布**：
>  - 例句：The new software was heavily advertised, but it underdelivered, lacking many of the promised features.
>  - 翻译：新软件广告做得非常好，但未能兑现，缺乏许多承诺的功能。
>
>例句分析
>
>- **句子中的 "overpromising and underdelivering"**：
>  - **解释**：人工智能承诺的远远超过实际能做到的，结果未能实现预期。
>  - **句子**：By the late 1980s AI had fallen into disrepute, a byword for overpromising and underdelivering.
>  - **翻译**：到20世纪80年代后期，人工智能已经声名狼藉，成为过度承诺和未能兑现的代名词。
>
>这个术语形象地描述了人工智能在那个时期的情况，虽然当时对人工智能的期望很高，但实际结果却未能达到预期，导致了失望和声誉受损。

原文：

It was from one of those pockets of perseverance that today’s boom was

born. As the rudiments of the way in which brain cells—a type of neuron—

work were pieced together in the 1940s, computer scientists began to

wonder if machines could be wired up the same way. In a biological brain

there are connections between neurons which allow activity in one to trigger

or suppress activity in another; what one neuron does depends on what the

other neurons connected to it are doing. A first attempt to model this in the

lab (by Marvin Minsky, a Dartford attendee) used hardware to model

networks of neurons. Since then, layers of interconnected neurons have been

simulated in software

今天的繁荣就是从这些坚持不懈的努力中诞生的。随着脑细胞(一种神经元)工作方式的雏形在20世纪40年代被拼凑起来，计算机科学家开始想知道机器是否可以以同样的方式连接。在生物大脑中，神经元之间存在联系，允许一个神经元的活动触发或抑制另一个神经元的活动；一个神经元做什么取决于与之相连的其他神经元在做什么。第一次在实验室对此建模的尝试(由马文·明斯基，一个达特福德的参加者)使用硬件来模拟神经元网络。从那时起，一层层相互连接的神经元就被用软件模拟出来了

学习：

perseverance: 美 [ˌpɜːrsəˈvɪrəns] 毅力；不屈不挠的精神；韧性 **注意发音**

rudiments：美 [ˈrudəmənts] 入门；基础知识；初步；（rudiment的复数）

pieced：连接；凑合；（piece的过去式和过去分词）          

were pieced together：连接起来

wire up：连接起来

pockets: 小团体

>解释 "Wire up" 和 "Pockets"
>
>**句子的意思**：
>- **Wire up**：连接起来，指的是将机器或设备按照某种方式连接起来。
>- **Pockets**：小团体或小区域，这里指的是在某个特定领域或地点坚持努力的人或团队。
>
>详细解析：
>
>- **Wire up**：这个短语常用来描述连接设备或系统的过程，就像将电线连接到一个电路板上。
>  - **例句**：
>    - Engineers wired up the new system to ensure it worked seamlessly with the existing infrastructure.
>    - 翻译：工程师们将新系统连接起来，以确保它能与现有基础设施无缝工作。
>  - **在句子中的解释**：
>    - **句子**：Computer scientists began to wonder if machines could be wired up the same way.
>    - **解释**：计算机科学家开始思考机器是否可以以类似的方式连接起来，就像神经元在生物大脑中的连接方式一样。
>
>- **Pockets**：指小团体、小区域或孤立的地方，通常用于描述特定的情况或特定领域中的一小部分。
>  - **例句**：
>    - There are still pockets of resistance against the new policy in some regions.
>    - 翻译：在某些地区仍然有小部分人反对新政策。
>  - **在句子中的解释**：
>    - **句子**：It was from one of those pockets of perseverance that today’s boom was born.
>    - **解释**：今天的繁荣是从那些坚持不懈的小团体或区域中诞生的。
>



原文：

These artificial neural networks are not programmed using explicit rules;

instead, they “learn” by being exposed to lots of examples. During this

training the strength of the connections between the neurons (known as

“weights”) are repeatedly adjusted so that, eventually, a given input

produces an appropriate output. Minsky himself abandoned the idea, but

others took it forward. By the early 1990s neural networks had been trained

to do things like help sort the post by recognising handwritten numbers.

Researchers thought adding more layers of neurons might allow more

sophisticated achievements. But it also made the systems run much more

slowly.

这些人工神经网络不是使用显式规则编程的；相反，他们通过接触大量的例子来“学习”。在这个训练过程中，神经元之间的连接强度(称为“权重”)被反复调整，以便最终给定的输入产生适当的输出。明斯基本人放弃了这个想法，但其他人继续推进。到20世纪90年代初，神经网络已经被训练成通过识别手写数字来帮助分拣邮件。研究人员认为，增加更多的神经元层可能会带来更复杂的成就。但是这也使得系统运行得更加缓慢。

原文：

A new sort of computer hardware provided a way around the problem. Its

potential was dramatically demonstrated in 2009, when researchers at

Stanford University increased the speed at which a neural net could run 70-

fold, using a gaming PC in their dorm room. This was possible because, as

well as the “central processing unit” (CPU) found in all PCs, this one also had a

“graphics processing unit” (GPU) to create game worlds on screen. And the 

GPU was designed in a way suited to running the neural-network code.

一种新的计算机硬件提供了解决这个问题的方法。它的潜力在2009年得到了显著地展示，当时斯坦福大学的研究人员在宿舍里使用游戏电脑，将神经网络的运行速度提高了70倍。这是可能的，因为除了所有PC中的“中央处理器”(CPU)，这台电脑还有一个“图形处理器”(GPU)来在屏幕上创建游戏世界。GPU的设计方式适合运行神经网络代码。

学习：

around: 绕过，避开

>解释 "around"
>
>**句子的意思**：
>- **Around**：在这里表示“绕过”或“避开”的意思，指的是找到一个方法来避免或解决某个问题。
>
>详细解析：
>
>- **Around** 这个词在不同的上下文中可以有多种解释。在这里，它用来描述找到一种方法来避免遇到的困难或问题。
>  - **例句**：
>    - The team found a way around the scheduling conflict by rescheduling the meeting.
>    - 翻译：团队通过重新安排会议找到了一个绕过时间冲突的方法。
>  - **在句子中的解释**：
>    - **句子**：A new sort of computer hardware provided a way around the problem.
>    - **解释**：一种新型的计算机硬件提供了一种绕过该问题的方法。
>
>在这个例子中，**around** 强调的是通过使用新技术或方法，成功地解决了之前的困难。

原文：

Coupling that hardware speed-up with more efficient training algorithms

meant that networks with millions of connections could be trained in a

reasonable time; neural networks could handle bigger inputs and, crucially,

be given more layers. These “deeper” networks turned out to be far more

capable.

将硬件加速与更有效的训练算法结合起来，意味着拥有数百万个连接的网络可以在合理的时间内得到训练；神经网络可以处理更大的输入，更重要的是，可以有更多的层次。这些“更深”的网络被证明更有能力。

学习：

capable：有能力的；能力强的；有才能的；

crucially：关键地；至关重要地

原文：

The power of this new approach, which had come to be known as “deep

learning”, became apparent in the ImageNet Challenge of 2012. Image

recognition systems competing in the challenge were provided with a

database of more than a million labelled image files. For any given word,

such as “dog” or “cat”, the database contained several hundred photos.

Image-recognition systems would be trained, using these examples, to

“map” input, in the form of images, onto output in the form of one-word

descriptions. The systems were then challenged to produce such descriptions

when fed previously unseen test images. In 2012 a team led by Geoff

Hinton, then at the University of Toronto, used deep learning to achieve an

accuracy of 85%. It was instantly recognised as a breakthrough.

这种新方法的力量，后来被称为“深度学习”，在2012年的ImageNet挑战赛中变得显而易见。参加挑战赛的图像识别系统配备了一个包含超过一百万个带标签图像文件的数据库。对于任何给定的词，如“狗”或“猫”，数据库包含数百张照片。使用这些例子，图像识别系统将被训练成把图像形式的输入“映射”到一个单词描述形式的输出上。这些系统被要求在之前没见过的测试图像时做出这样的描述。2012年，由当时在多伦多大学的Geoff Hinton领导的团队使用深度学习达到了85%的准确率。这立即被认为是一个突破。

原文：

By 2015 almost everyone in the image-recognition field was using deep

learning, and the winning accuracy at the ImageNet Challenge had reached

96%—better than the average human score. Deep learning was also being

applied to a host of other “problems…reserved for humans” which could be

reduced to the mapping of one type of thing onto another: speech

recognition (mapping sound to text), face-recognition (mapping faces to

names) and translation.

到2015年，图像识别领域的几乎每个人都在使用深度学习，ImageNet挑战赛的获胜准确率达到了96%——高于人类的平均得分。深度学习还被应用于许多其他“为人类保留的问题”,这些问题可以被简化为一种类型的事物到另一种类型的映射:语音识别(将声音映射到文本)、面部识别(将面部映射到姓名)和翻译。

原文：

In all these applications the huge amounts of data that could be accessed

through the internet were vital to success; what was more, the number of

people using the internet spoke to the possibility of large markets. And the

bigger (ie, deeper) the networks were made, and the more training data they

were given, the more their performance improved.

在所有这些应用中，可以通过互联网访问的大量数据对成功至关重要；此外，使用互联网的人数表明了巨大市场的可能性。网络越大(也就是越深)，给它们的训练数据越多，它们的表现就越好。

学习：

spoke to: 暗示，表明

>解释 "spoke"
>
>**句子的意思**：
>
>- **Spoke**：在这里表示“暗示”或“表明”的意思，指的是互联网用户的数量表明了潜在市场的规模。
>
>详细解析：
>
>- Spoke
>
>   这个词在不同的上下文中可以有多种解释。在这里，它用来描述某个现象或事实所暗示的信息。
>
>  - 例句
>
>    ：
>
>    - His success in multiple projects spoke to his expertise in the field.
>    - 翻译：他在多个项目中的成功表明了他在该领域的专业知识。

原文：

Deep learning was soon being deployed in all kinds of new products and

services. Voice-driven devices such as Amazon’s Alexa appeared. Online

transcription services became useful. Web browsers offered automatic

translations. Saying such things were enabled by AI started to sound cool,

rather than embarrassing, though it was also a bit redundant; nearly every

technology referred to as AI then and now actually relies on deep learning

under the bonnet.

深度学习很快被部署在各种新产品和服务中。亚马逊的Alexa等语音驱动设备出现了。在线转录服务变得很有用。网络浏览器提供自动翻译。说这些事情是由人工智能实现的开始听起来很酷，而不是尴尬，尽管这也有点多余；几乎所有过去和现在被称为人工智能的技术实际上都依赖于深度学习。

学习：

automatic translation：自动翻译

bonnet：美 [ˈbɑːnɪt] （车辆的）引擎盖；引擎罩；



**ChatGPT** **and its rivals really do seem to “use language and form abstractions”**

原文：

In 2017 a qualitative change was added to the quantitative benefits being

provided by more computing power and more data: a new way of arranging

connections between neurons called the transformer. Transformers enable

neural networks to keep track of patterns in their input, even if the elements

of the pattern are far apart, in a way that allows them to bestow “attention”

on particular features in the data.

2017年，更强的计算能力和更多的数据所提供的定量优势增加了一个质变:一种排列神经元之间连接的新方法，称为transformer。transformer使神经网络能够跟踪其输入中的模式，即使模式的元素相距很远，这种方式允许它们对数据中的特定特征给予“关注”。

学习：

qualitative：美 [ˈkwɑːlɪteɪtɪv] 性质上的；质量上的；定性的； **注意发音**

qualitative change：质变

bestow：美 [bɪˈstoʊ] 授予；（将…）给予；赠送；

原文：

Transformers gave networks a better grasp of context, which suited them to

a technique called “self-supervised learning”. In essence, some words are

randomly blanked out during training, and the model teaches itself to fill in

the most likely candidate. Because the training data do not have to be

labelled in advance, such models can be trained using billions of words of

raw text taken from the internet

Transformers 让网络更好地掌握了上下文，这使它们适应了一种叫做“自我监督学习”的技术。本质上，一些单词在训练过程中被随机删除，模型自学填充最有可能的候选词。因为训练数据不需要提前标注，所以这种模型可以使用从互联网上获取的数十亿字的原始文本进行训练

## **Mind your language model**

原文：

Transformer-based large language models (LLMs) began attracting wider

attention in 2019, when a model called GPT-2 was released by OpenAI, a startup

(GPT stands for generative pre-trained transformer). Such LLMs turned out to be

capable of “emergent” behaviour for which they had not been explicitly

trained. Soaking up huge amounts of language did not just make them

surprisingly adept at linguistic tasks like summarisation or translation, but

also at things—like simple arithmetic and the writing of software—which

were implicit in the training data. Less happily it also meant they reproduced

biases in the data fed to them, which meant many of the prevailing

prejudices of human society emerged in their output.

基于Transformer的大型语言模型(LLM)在2019年开始吸引更广泛的关注，当时初创公司OpenAI发布了一种名为GPT-2的模型(GPT代表生成式预训练Transformer)。事实证明，这种LLM能够做出“涌现”行为，而它们并没有受过这方面的明确训练。吸收大量的语言不仅使他们在总结或翻译等语言任务上出奇的熟练，而且在一些事情上也是如此——比如简单的算术和软件编写——这些都隐含在训练数据中。不太令人高兴的是，这还意味着他们在提供给他们的数据中复制了偏见，这意味着人类社会的许多普遍偏见出现在他们的输出中。

原文：

In November 2022 a larger OpenAImodel, GPT-3.5, was presented to the public in

the form of a chatbot. Anyone with a web browser could enter a prompt and

get a response. No consumer product has ever taken off quicker. Within

weeks ChatGPT was generating everything from college essays to computer

code. AI had made another great leap forward.

2022年11月，一个更大的开放模型GPT 3.5以聊天机器人的形式呈现在公众面前。任何有网络浏览器的人都可以输入提示并得到响应。没有哪种消费产品比这更快起飞了。几周之内，ChatGPT就生成了从大学论文到计算机代码的各种东西。AI又向前跃进了一大步。

原文：

Where the first cohort of AI-powered products was based on recognition, this

second one is based on generation. Deep-learning models such as Stable

Diffusion and DALL-E, which also made their debuts around that time, used a

technique called diffusion to turn text prompts into images. Other models

can produce surprisingly realistic video, speech or music.

第一批人工智能产品是基于识别，而第二批是基于生成。Stable Diffusion和DALL-E等深度学习模型也是在那个时候首次亮相，它们使用了一种称为扩散的技术，将文本提示转化为图像。其他模型可以产生令人惊讶的逼真的视频、语音或音乐。

学习：

cohort：美 [ˈkoʊhɔːrt] （有共同特点或举止类同的）一群人；一批人； 生物学中的）群体

原文：

The leap is not just technological. Making things makes a difference. ChatGPT

and rivals such as Gemini (from Google) and Claude (from Anthropic,

founded by researchers previously at OpenAI) produce outputs from

calculations just as other deep-learning systems do. But the fact that they

respond to requests with novelties makes them feel very unlike software

which recognises faces, takes dictation or translates menus. They really do

seem to “use language” and “form abstractions”, just as McCarthy had

hoped.

这一飞跃不仅仅是技术上的。让事物产生大影响。ChatGPT和竞争对手如Gemini(来自谷歌)和Claude(来自Anthropic，由OpenAI的研究人员创建)从计算中产生输出，就像其他深度学习系统一样。但事实上，他们用新奇的事物来回应请求，这让他们觉得自己非常不像识别面孔、记录口述或翻译菜单的软件。他们似乎真的“使用语言”和“形成抽象”，正如麦卡锡所希望的那样。

原文：

This series of briefs will look at how these models work, how much further

their powers can grow, what new uses they will be put to, as well as what

they will not, or should not, be used for. ■

这一系列的简报将着眼于这些模型是如何工作的，它们的能力还可以增长到什么程度，它们将被用于什么新的用途，以及它们不会或不应该用于什么。■



## 后记

2024年7月27日17点53分于上海。



